{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The three different types of machine learning:\n",
    "  \n",
    "Taking a look at the three types of machine learning: supervised learning, unsupervised learning, and reinforcement learning.  \n",
    "  \n",
    "### Supervised learning:\n",
    "The main goal in supervised learning is to learn a model from labeled training data that allows us to make predictions about unseen or future data.  \n",
    "![image](./pics/supervised.png)  \n",
    "Here, the term supervised refers to a set of samples where the desired ouput signals (labels) are already known.  \n",
    "Considering the example of e-mail spam filtering, we can train a model using a supervised machine learning algorithm on a corpus of labeled e-mail, e-mail that are correctly marked as spam or not spam, to predict whether a new e-mail belongs to either of the two categories. A supervised learning task with discrete *class labels*, such as in the previous e-mail spam-filtering example, is also called *classification* task. Another subcategory of supervised learning is *regression*, where the outcome signal is a continuous value.  \n",
    "#### Classification for predicting class labels\n",
    "Classification is a subcategory of supervised learning where the goal is to predict the categorical class labels of new instances based on past observations. Those class labels are dicrete, unordered values that can be understood as the *group memberships* of the instances. The previously mentioned example of e-mail-spam detection represents a typical ecample of binary classification task, where the machine learning algorithm learns a set of tules in order to distinguish between two possible classes: spam and non-spam e-mail.  \n",
    "However, it doesn't have to be of a binary nature. A typical example of a *multi-class classification* task is handwritten character recognition. Here, we could collect a training dataset that consists of multiple handwritten examples of each letter in the alphabet. Now, if an user provides a new handwritten character via an input device, our predictive model will be able to predict the correct letter in the alphabet with certain accuracy. However, our machine learning system would be unable to correctly recognize any of the digits zero to nine, for example, if they were not part of our training dataset.\n",
    "#### Regression for predicting continuous outcomes\n",
    "We learned in the previous section that the task of classification is to assign categorical, unordered labels to instances. A second type of supervised learning is the prediction of continuous outcomes, which is also called regression analysis. In *regression analysis*, we are given a number of *predictor* (explanatory) variables and a continuous response variable (outcome), and we try to find the relationship between those variables that allows us to predict an outcome.  \n",
    "For example, let's assume that we are interested in predicting the Math SAT scoes of our students. If there is a relationship between the time spent studying for the test and the final scores, we could use it as training data to learn a model that uses the study time to predict the test scores of future students who are planning to take this test (for example, lenear regression).  \n",
    "\n",
    "### Reinforcement learning:\n",
    "Another type of machine learning is reinforcement leanring. In reinforcement learning, the goal is to develop a system (*agent*) that improces its performance based on interactions with the *environment*. Since the information about the current state of the environment typically also includes a so-called *reward* signal, we can think of reinforcement learning as a field related to *supervised* learning. However, in reinforcement learning this feedback is not the correct ground truth label or value, but a measure of how well the action was measured by a *reward* function. Through the interaction with the environment, an agent can then use reinforcement leanring to learn a series of actions that maximizes this reward via a exploratory trail-and-error approach or deliberative planning.  \n",
    "A popular example of reinforcement learning is a chess engine. Here, the agent decides upon a series of moves depending on the state of the board (the environment), and the reward can be defined as *win* or *lose* at the end of the game:  \n",
    "![image](./pics/reinforcement.png)  \n",
    "\n",
    "### Unsupervised learning:\n",
    "In supervised learning, we know the *right answer* beforehand when we train our model, and in reinforcement learning, we define a measure of *reward* for particular actions by the agent. In unsupervised learning, however, we are dealing with unlabeled data or data of *unknown structure*. Using unsupervised learning techniques, we are able to explore the structure of our data to extract meaning ful information without the guidance of a known outcome variable or reward function.  \n",
    "#### Finding subgroups with clustering\n",
    "*Clustering* is an exploratory data analysis technique that allows us to organize a pile of information into meaningful subgroups (*clusters*) without having any prior knowledge of their group memberships. Each cluster that may arise during the analysis defines a group of objects that shre a certain degree of similarity but are more dissimilar to objects in other clusters, which is why clustering is also sometimes called \"unsupervised classification.\" Clustering is a great technique for structuring information and deriving meaningful relationships among data. For example, it allows marketers to dicover customer groups based on their interests in order to develop distinct marketing programs.  \n",
    "#### Dimensionality reduction for data compression\n",
    "Another subfield of unsupervised learning is *dimensionality reduction*. Often we are working with data of high diensionality - each observation comes with a high number of measurements - that can present a challenge for limited storage space and the computational performance of machine learning algorithms. Unsupervised dimensionality reduction is a commonly used approach in feature preprocessing to remove noise from data, which can also degrade the predictive performance of certain algorithms, and compress the data onto a smaller dimensional subspace while retaining most of the relevant information.  \n",
    "Sometimes, dimensionality reduction can also be useful for visualizing data - for example, a high-dimensional feature set can be projected onto one-, two-, or three-dimensional feature spaces in order to visualize it via 3D- or 2D-scatterplots or histograms.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An introduction to the basic terminology and notations\n",
    "\n",
    "To keep the noation and implementaiton simple yet efficient, we will make use of some of the basics of *linear algebra*. In the following chapters, we will use a *matrix* and *vector* notation to refer to our data. We will follow the common convention to represent each sample as separate row in a feature matrix __X__, where each feature is stored as a separate column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A roadmap for building machine learning systems\n",
    "In the previous sections, we discussed the basic concepts of machine learning and the three different types of learning. In this section, we will discuss other important parts of machine learning system accompanying the elarning algorithm. The diagram below shows a typical workflow diagram fo rusing machine learning in *predicative modeling*, which we will discuss in the following subsections:  \n",
    "![image](./pics/predicative.png)  \n",
    "### Preprocessing - getting data into shape\n",
    "Raw data rarely comes in the form and shape that is necessary for the optimal performance of a learning algorithm. Thus, the *preprocessing* of the data is one of the most crucial steps in any machine learning application. If we take the Iris flower dataset (common dataset in machine learning) as an example, we could think of the raw data as a series of flower images from which we want to extract meaningful feaures. Useful features could be the color, the hue, the intensity of the flowers, the height, and the flower lengths and widths. Many machine leaning algorithms also require that the selected featuers are on the same scale for optimal performance, which is often achieved by transforming the features in the range [0,1] or a standard normal disrtibution with zero  mean and unit variance, as we will see in the later chapters (feature scaling).  \n",
    "Some of the selected features may be highly correlated and therefore redundant to a certain degree. In those cases, dimensionality reduction techniques are useful for compressin the feature space has the advantage that less storage space is required, and the learning algorithm can run much faster.  \n",
    "To determine whether our machine learning algorithm not only performs well on the training set but also genrealizes well to new data, we also want to randomly divide the dataset into a separate training and test set. We  use the training set to train and optimize our machine learning model, while we keep the test set until the very end to evaluate the final model.  \n",
    "### Training and selecting a predictive model\n",
    "As we will see in later cahpters, many different machine learning algorithms have been developed to solve different problem tasks. An important point that can be summarized from David Wolpert's famous [*No Free Lunch Theorems*](https://en.wikipedia.org/wiki/No_free_lunch_theorem) is that we can't get learning \"for free\". Intuitively, we can relate this concept to the popular saying, \"*I suppose it is tempting, if the only tool you have is a hammer, to treat everythin as if it were a nail*\" (Abraham Maslow, 1966). For example, each classification algorithm has its inherent baises, and no single classification model enjoys superiority if we don't make any assumptions about the task. In practice, it is therefore essential to compare at least a handful of different algorithms in order to train and select the best performing model. But before we can compare different models, we first have to decide upon a metric to measure performance. One commonly used metric is classification accuracy, which is defined as the proportion of correctly classified instances.  \n",
    "One legitimate question to ask is: *how do we know which model performs well on the final test dataset and real-world data if we don't use this test set for the model selection but keep it for the final model evalution?* In order to address the issue embedded in this question, different cross-validation techniques can be used where the training dataset is further divided into training and *validation subsets* in order to estimate the *generalization performance* of the model. Finally, we also cannot except that the default paramters of the different learning algorithms provided by software libraries are optimal for our specific problem task. Therefore, we will make frequent use of hyperparameter *optimization techniques* that help us to fine-tune the performance of our model in later chapters. Intuitively, we can think of those hyperparameters as parameters that are not learned from the data but represent the knobs of a model that we can turn to improve its performance, which will become much clearer in later chapters when we see actual examples.  \n",
    "### Evaluating models and predicting unseen data instanes\n",
    "After we have selected a model that has been fitted on the training dataset, we can use the test dataset to estimate how well it performs on this unseen data to estimate the generalization error. If we are satisfied with its performance, we can now use this model to predict new, future data. It is important to note that the parameters for the previously mentioned procedures - such as feature scaling and dimensionality reduction - are solely obtained from the training dataset, and the same parameters are later re-applied to transform the test dataset, and the same parameters are later re-applied to transform the test dataset, as well as any new data samples - the performance measured on the test data may be overoptimistic otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Python for machine learning\n",
    "#### Python versions\n",
    "We will be using Python >= 3.4.3, and is recommended for you to use python3. Some of the python packages we will be using are:\n",
    "- NumPy 1.9.1\n",
    "- SciPy 0.14.0\n",
    "- scikit-learn 0.15.2\n",
    "- matplotlib 1.4.0\n",
    "- pandas 0.15.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
